# 概述
Kubernetes是一个全新的基于容器技术的分布式架构方案。Kubernetes具有完备的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制，以及多粒度的资源配额管理能力。同时还提供了完善的管理工具，这些工具涵盖了包括开发、部署测试、运维监控在内的各个环节。因此kubernetes是一个全新的基于容器技术的分布式架构解决方案，并且是一个一站式的完备的分布式系统开发和支撑平台。

# 快速安装kubernetes单机版环境
```
$ systemctl disable firewalld
$ systemctl stop firewalld
$ yum install -y etcd kubernetes
$ systemctl start etcd
$ systemctl start docker
$ systemctl start kube-apiserver
$ systemctl start kube-controller-manager
$ systemctl start kube-scheduler
$ systemctl start kubelet
$ systemctl start kube-proxy
```

# 基本概念和术语
k8s中大部分概念如Node、Pod、Replication Controller、Service等都可以看作一种“资源对象”；几乎所有的资源对象都可以通过k8s提供的kubectl工具(或API编程调用)执行增删改查等操作并将其保存在etcd中持久化存储。

## Mster
- k8s中的的Master指的是集群控制节点，每个k8s集群里需要有一个Master节点来负责整个集群的管理和控制。Master节点通常会独立占用一个服务器(高可用部署建议用3台)。

- Master节点上运行着一组关键进程：
    - Kubernetes API Server(kube-apiserver)：提供了HTTP Rest接口的关键服务进程，是k8s里所有资源的增删改查等操作的唯一入口，也是集群控制的入口进程。
    - Kubernetes Controller Manager(kube-controller-manager)：k8s里所有资源对象的自动化控制中心，可以理解为资源对象的"大总管"。
    - Kubernetes Scheduler(kube-scheduler)：负责资源调度(pod调度)的进程。

## Node
- 除了Master，k8s集群中的其他机器被称为Node节点，早期版本被称为Minion。与Master一样，Node节点可以是一台物理机，也可以是一台虚拟机。Node节点是k8s集群中的工作负载节点，当节点宕机时，工作负载(docker)会被Master自动转移到其他节点上。

- 每个Node节点都运行这一组关键进程：
    - Kubelet：负责Pod对应的容器的创建、启停等任务，同时与Master节点密切协作，实现集群管理的基本功能。
    - Kube-proxy：实现Kubernetes Service 的通信与负载均衡机制的重要组件。
    - Docker Engine(docker)：Docker引擎，负责本机的容器创建和管理工作。

- Node节点可以在运行期间动态增加到Kubernetes集群中，前提是这个节点已经正确安装配置和启动上述进程。默认情况下Kubelet会向Master注册自己，这也是Kubernetes推荐的Node管理方式。Node被纳入集群管理范围后，就会定时向Master节点汇报自己的资源使用情况实现高效均衡的资源调度策略。Node超时不上报信息，Msater直接判定为失联并标记为不可用(Not Ready)，随后Master集群会触发“工作负载大转移”的自动流程。

```
$ kubectl get nodes   //查看集群中有哪些节点
$ kubectl describe node <node_name>   //查看某个Node的详细信息
```

- Node详细信息包括：
    - Node基本信息：名称、标签、创建时间等。
    - Node当前的运行状态，Node启动后会做一些自检操作磁盘、内存等。如果一切正常就设置为Ready状态。该状态表示Node处于健康状态，Master就可以在上面调度新的任务。
    - Node的主机地址与主机名。
    - Node上的资源总量：描述Node可用的系统资源，包括CPU、内存数量、最大可调度Pod数量等。
    - Node可分配资源量：描述Node当前可用于分配的资源量。
    - Node主机系统信息：包括主机的唯一标识UUID、Linux Kernel版本号、操作系统类型与版本、Kubernetes版本号、Kubelet与Kube-proxy的版本号等。
    - Node当前正在运行的Pod列表概要信息。
    - Node已分配的资源使用概要信息，例如资源申请的最低、最大允许使用量占系统总量的百分比。
    - Node相关的Event信息。

## Pod
- Pod是Kubernetes的最重要也最基本的概念。是一组容器作为一个单元。每个Pod都有一个特殊的被称为"根容器"的Pause容器。Pause容器对应的镜像属于Kubernetes平台的一部分，它的状态代表了这个容器组的状态。除了Pause容器，每个Pod还包含了一个或多个紧密相关的用户业务容器。多个业务容器共享Pause容器的IP，共享Pause容器挂接的Volume。

- Kubernetes为每个Pod都分配了唯一的IP地址，称之为Pod IP，一个Pod里的多个容器共享Pod IP地址。Kubernetes要求底层网络支持集群内任意两个Pod之间的TCP/IP直接通信，通常采用虚拟二层网络技术实现、例如Flannel、Open vSwitch等。因此在Kubernetesl里，一个Pod里的容器与另外主机上的Pod容器能够直接通信。

- Pod有两种类型：普通的Pod及静态Pod(Staic Pod):
    - 静态Pod比较特殊、并不存放在Kubernetes的etcd存储里，而是放在某个具体的Node上的一个具体文件中，并且只在此Node节点上启动运行。
    - 普通Pod一旦被创建，就会被放入etcd中存储，随后会被Kubernetes Master调度到某个具体的Node上并进行绑定(Binding),随后该Pod被对应的Node上的Kubelet进程实例化成一组相关Docker容器并启动起来。在默认情况下、当Pod里的某个容器停止，Kubernetes会自动检测并重启这个Pod里的所有容器，如果Pod所在的Node宕机，则会将这个Node上的所有Pod重新调度到其他健康节点上。

- Kubernetes里的所有资源对象都可以采用yaml或者JSON格式的文件来定义。Pod IP和资源定文件里的容器端口(containerPort)组成一个新的概念-Endpoint。用来对外通信。一个Pod可以存在多个Endpoint。

- Docker Volume在Kubernetes里对应着Pod Volume，后者有一些扩展，比如用分布式文件系统GlusterFS实现后端存储功能。Pod Volume是定义在Pod之上，然后被各个容器挂载到自己的文件系统中。

- Kubernetes的Event概念，Event是一个事件的记录，记录了事件的最早产生时间、最后重现时间、重复次数、发起者、类型、以及导致此事件的原因等信息。Event通常会关联到某个具体的资源对象上，是排查故障的重要参考信息。Pod同样有Event记录、当发现Pod无法被创建时、就可以用`Kubectl descirbe pod <pod_name>`来查看错误信息。

- 每个Pod可以对其能使用的服务器上的计算资源设置CPU与Memory限额。CPU限额的资源单位为CPU(Core)的数量，是一个绝对值。1个CPU的配置对大部分容器来说是一个相当大的资源配额，所以在k8s里通常以千分之一的CPU配额为最小单位，用m来表示，通常一个容器的CPU配额被定义为100~300m，即占用0.1~0.3个CPU。与CPU配额类似，Memory配额也是一个绝对值，单位是内存字节数。 在Kubernetes里，一个计算资源进行配额限定需要设定两个参数：
    - Requests：该资源的最小申请量，系统必须满足要求。通常会设置一个比较小的数值，符合容器平时的工作负载情况下的资源需求。
    - Limits：该资源最大使用的量、不能被突破，当系统试图超过这个设定、可能会被k8s Kill并重启。通常会设置为峰值负载情况下占用的最大量。

## Label
- Label是Kubernetes系统中另外一个核心概念。一个Label是一个key=value的键值对，其中key与value由用户自定义。Label可以附加到各种资源对象上，一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意资源数量的资源对象上去。Label通常在资源对象定义是定义，可以在对象创建后动态添加或者删除。

- 常用Label示例：
    - 版本标签："release":"stable","release":"canary"
    - 环境标签："environment":"dev","environment":"qa","environment":"production"
    - 架构标签："tier":"frontend","tier":"backend","tier":"middleware"
    - 分区标签："partition":"customerA","partition":"customerB"
    - 质量管控标签："track":"daily","track":"weekly"


- 给某个资源定义了Label后，就可以通过Label Selector(标签选择器)查询和筛选拥有某些Label的资源对象。有两种Label Selector 表达式：
    - 基于等式(Equality-based)
        - name = redis-slave 匹配所有具有标签name=redis-slave的资源对象
        - env != production 匹配所有不具有此标签的资源对象


    - 基于集合(Set-based)
        - name in (redis-master, redis-salave)
        - name not in (php-fronted)

> 可以通过多个Label Selector表达式的组合实现复杂的条件选择，多个表达式之间用","进行分割，几个条件之间是"AND"的关系。

- 新出现的管理对象如Deployment、ReplicaSet、DaemonSet和Job则可以在Selector中使用基于集合的筛选条件：

    ```
    selector:
      matchLabels:
        app: myweb
      matchExpressions:
        - {key: tier, operator: In, values: [frontend]}
        - {key: environment, operator: NotIn, values: [dev]}
    ```

    - matchLabels 用于定义一组Label，与直接写在Selector中作用相同；
    - matchExpressions 用于定义一组基于集合的筛选条件、可用的条件运算符包括： In、NotIn、Exists 和 DoesNotExist
    - 如果同时设置了matchLabels和matchExpressions，则两组条件为"AND"关系

- Label Selector在Kubernetes中的使用场景：
    - kube-controller进程通过资源对象RC上定义的Label Selector来筛选要监控的Pod副本的数量，从而实现Pod副本的数量始终符合预期设定的全自动控制流程。
    - kube-proxy进程通过Service的Label Selector来选择对应的Pod，自动建立每个Service到对应的Pod的请求转发路由表，从而实现Service的智能负载均衡机制。
    - 通过对某些Node定义特定的Label，并且在Pod定义文件中使用NodeSelector这种标签调度策略，kube-scheduler进程可以实现Pod"定向调度"的特征

- 使用Label可以给对象创建多租标签，Label和Label Selector共同构成了Kubernetes系统中最核心的应用模型，使得被管理对象能够被精细的分组管理，同时实现了整个集群的高可用性。

## Replication Controller(RC)
- RC是Kubernetes系统中的核心概念之一，简单来说，它其实是定义了一个期望的场景，即声明某种Pod的副本数量在任意时刻都符合某个预期值，RC的定义包括下面3个部分：
    - Pod期待的副本数(replicas)
    - 用于筛选目标Pod的Label Selector
    - 当Pod的副本数量小于预期数量时，用于创建新Pod的Pod模板(template)

- 通过RC，Kubernetes实现了集群的高可用性，当定义了一个RC提交到Kubernetes集群后，Master节点上的Controller Manager组件就会得到通知，定期巡检系统中当前存活的Pod，并确保目标Pod示例数量刚好等于RC的期望值，系统会自动停掉过多的Pod或者自动创建缺少的Pod。

- 在运行时，可以通过修改RC的副本数量，来实现Pod的动态缩放(Sccaling)功能`kubectl scale rc <pod_name> --replicas=3`,同时删除RC也不会影响该RC创建好的Pod，也可以通过设置replicas的值为0，然后更新RC的方法删除所有Pod。此外kubectl提供了stop和delete命令来一次性删除RC和RC控制的全部Pod。

> 由于Replication Controller 与 Kubernetes 代码中的模块 Replication Controller 同名，在 Kubernetes v1.2版本升级为一个新的概念 Replica Set。

- RC(Replica Set)的特性与作用：
    - 在大多数情况下，通过定义一个RC实现Pod的创建过程及副本数量的自动控制；
    - RC里包括完整的Pod定义模板；
    - RC通过Label Selector机制实现对Pod副本的自动控制；
    - 通过改变RC里的Pod副本数量，可以实现Pod的扩容或缩容功能；
    - 通过改变RC里Pod模板中的镜像版本，可以实现Pod的滚动升级(rolling update)功能；

## Deployment
- Deployment 是 Kubernetes v1.2引入的概念，引入的目的是为了更好的解决Pod的编排问题。为此Deployment在内部使用了Replica Set来实现。无论从Deployment的作用与目的，它的YAML定义，还是命令操作看都可以看做是RC的一次升级。

- Deployment相对于RC最大的一个升级是我们随时可以知道当前Pod"部署"的进度。

- Deployment的典型应用场景：
    - 创建一个Deployment对象来生成对应的Replica Set并完成Pod副本的创建过程；
    - 检查Deployment的状态来看部署动作是否完成，(Pod副本数量是否达到预期的值)；
    - 更新Deployment已创建新的Pod(镜像升级)；
    - 如果当前Deployment不稳定，回滚到上一个Deployment版本；
    - 暂停Deployment以便于一次性修改多个PodTemplateSpec的配置项，之后恢复并进行新的发布；
    - 扩展Deployment以应对高负载；
    - 查看Deployment的状态，以此作为发布是否成功的指标；
    - 清理不再查看的旧版本ReplicaSets；

- Deployment的定义与Replica Set的定义很类似，除了API声明与Kind类型有所区别：
```
apiVersion: extensions/v1beta1             apiVersion: v1
kind: Deployment                           kind: ReplicaSet
metadata:                                  metadata:
  name: nginx-deployment                     name: nginx-repset

$ kubectl create -f tomcat-deployment.yaml    //创建Deployment
$ kubectl get deployments  //查看Deployment的信息

# DESIRED：Pod副本数量的期望值，及Deployment里定义的Replica
# CURRENT：当前Replica的值，当这个值等于DESIRED的值表示整个部署过程完成
# UP-TO-DATE：最新版本的Pod的副本数量，用于指示在滚动升级过程中，有多个Pod副本已经成功升级
# AVAILABLE：当前集群中存活的Pod数量

$ kubectl get rs    //查看对应的Replica Set
$ kubectl get Pod   //查看Pod

# 创建Pod的命名以Deployment对应的Replica Set的名字为前缀，清晰地表明一个Replica Set创建了那些Pod，容易排查错误

$ kubectl descirbe deployments    //查看Deployment控制的Pod的水平扩展过程
```

- Pod的管理对象除了RC和Deployment，还包括ReplicaSet、DaemonSet、SatefulSet、Job等。分别应用到不同的场景。

## Horizontal Pod Autoscaler(HPA)
 [hɒrɪ'zɒnt(ə)l] [pɒd] ['ɔːtəʊ] ['skelɚ] Pod横向扩容

- HPA 与 RC、Deployment一样属于一种 Kubernetes资源对象。通过追踪分析RC控制的所有目标Pod的负载变化情况，来确定是否需要针对性地调整目标Pod的副本数。HPA有两种方式作为Pod负载的度量指标：
    - CPUUtilizationPercentage
        - CPUUtilizationPercentage是一个算数平均值，即所有副本自身的CPU利用率(Pod当前CPU使用量除以Pod Request)的平均值。通常是查询Heapster扩展组件获取CPU1min内的平均值，所以需要安装和部署Heapster。
    - 应用程序自定义的度量指标，比如服务在每秒内的相应的请求数(TPS或QPS)

## StatefulSet
- 在Kubernetes系统中，Pod的管理对象RC、Deployment、DaemonSet和Job都是面向无状态的服务(没有固定IP、名字、ID等名字是随机生产、IP是启动生成并且不固定、恢复集群中失败节点也需要挂载共享存储)。为了解决这些问题k v1.4版本引入PetSet资源对象，在v1.5更名为StatefulSet，从本质上说，可以看做Deployment/RC的一个特殊变种，有如下特性：
    - StatefulSet里每个Pod都有稳定、唯一的网络标识，可以用来发现集群内的其他成员。
    - StatefulSet控制的Pod副本的启停顺序是受控的，操作第n个Pod时，前n-1个Pod已经是运行且准备好的状态
    - StatefulSet里的Pod采用稳定的持久化存储卷，通过PV/PVC来实现，删除Pod时默认不删除与StatefulSet相关的存储卷

- StatefulSet除了要与PV卷捆绑使用以存储Pod的状态数据，还要与Headless Service配合使用，即在每个StatefulSet的定义中要声明它属于哪个Headless Service。Headless Service与普通Service的关键区别在于，他没有Cluster IP，如果解析Headless Servicede DNS域名，则返回的是改Service对应的全部Pod的Endpoint列表。StatefulSet在Headless Service的基础上又为StatefulSet控制的每个Pod实例创建了一个DNS域名，这个域名的格式为`$(podname).$(headless service name)`

## Service(服务)
### 概述
- Service也是Kubernetes里的最核心的资源对象之一，kubernetes里的每个Service其实就是微服务架构中的一个"微服务"，之前的Pod、RC等资源对象都是为Service做"嫁衣的"的。通过分析、识别并建模系统中的所有服务为微服务，最终的系统由多个提供不同业务能力而又彼此独立的微服务单元所组成，服务之间通过TCP/IP进行通信。从而形成拥有了强大的分布式能力、弹性扩展能力、容错能力的弹性网络。

- Kubernetes的负载均衡机制也遵循了常规nginx的做法，运行再每个Node上的kube-proxy进程就是一个只能的软件负载均衡器，它负责把对Servide的请求转发到后端的某个Pod实例上，并在内部实现服务的负载均衡与会话保持机制。而且Kubernetes还发明了一种很巧妙又影响深远的设计：Service不是共用一个负载均衡器的IP地址，而是每个Service分配了一个全局唯一的虚拟IP地址，这个虚拟IP被称为Cluster IP，这样每个服务就变成了具备唯一IP地址的通信节点，服务调用就变成了最基础的TCP网路通信问题。

- Pod的Endpoint地址会随着Pod的销毁和重建发生变化，而Cluster IP在Service的整个生命周期不会发生变化。在Kubernetes架构里通过用Service的Name与Service的Cluster IP做一个DNS域名映射即可完美解决问题。

```
# 相关命令

$ kubectl create -f tomcat.yaml   //创建一个Service
$ kubectl get endpoints   //查看Service的Endpoint列表
$ kubectl get svc tomcat -o yaml    //查看Service更多信息
```

- 在Service中spec.ports的定义中，targetPort属性用来确定提供该服务的容器所暴露的端口；而port属性则定义了Service的虚拟端口。如果没有指定targetPort，则默认targetPort与port相同。

### Kubernetes的服务发现机制
每个Kubernetes中的Service都有一个唯一的ClusterIP及唯一的名字，名字是自定义在配置中。如何通过Service的名字找到Cluster IP，早期是通过环境变量解决，目前都是采取DNS这种服务发现机制。

### 外部系统访问Service的问题
- Kubernetes里的"三种IP"：
    1. Node IP：Node节点的IP；
        - Node IP是Kubernetes集群中每个节点的物理网卡的IP地址，Kubernetes集群之外的节点访问集群之内的节点都是必须通过Node IP进行通信。
    2. Pod IP：Pod的ip地址；
        - Pod IP是每个Pod的IP地址，它是Docker Engine根据docker0网桥的IP地址段进行分配的，通常是一个虚拟的二层网络。Kubernetes里一个Pod里的容器访问另外一个Pod里的容器，就是通过Pod IP所在的虚拟二层网络进行通信的，而真实的TCP/IP流量则是通过Node IP所在的物理网卡流出的。
    3. Cluster IP：Service的IP地址；
        - 一个伪造的虚拟IP，原因如下：
            - Cluster IP仅仅作用于Kubernetes Service这个对象，并由Kubernetes管理和分配IP地址(来源于Cluster地址池)
            - Cluster IP无法被ping，因为没有一个"实体网络对象"来响应
            - Cluster IP只能结合Service Port组成一个具体的通信端口，单独的Cluster IP不具备TCP/IP通信的基础，并且他们属于Kubernetes集群这样一个封闭的空间，集群之外的节点要访问这个通信端口，需要做一些额外的工作
            - 在Kubernetes集群之内，Node IP网、Pod IP网与Cluster IP网之间的通信，采用的是Kubernetes自己设计的一种编程方式的特殊的路由规则，与我们熟知的IP路由大不相同。

    ```
    # Service定义：

    spec:
      type: NodePort          *
    ports:
      - port: 8080
        nodePort: 31002       *
      selector:
        tier: frontend

    # 将Service的8080端口映射到NodeIP:31002
    ```

## volume(存储卷)
- Volume是Pod中能够被多个容器访问的共享目录。Kubernetes中的Volume定义在Pod上，然后被一个Pod里的多个容器挂载到具体的文件目录下；Kubernetes中的Volume与Pod的生命周期相同，但与容器的生命周期不相关；容器终止或重启时，Volume中的数据也不会丢失。Kubernetes还支持多种类型的Volume，例如GlusterFS、Ceph等先进的分布式文件系统。

- Volume的使用，现在Pod上声明一个Volume，然后在容器里引用该Volume并Mount到容器里的某个目录上。

    ```
    # Pod定义：

    spec:
      volumes:                             *
      - name: datavol                      *
        emptyDir: {}                       *
      containers:
      - name: tomcat-demo
        image: tomcat
        volumeMounts:                      *
          - mountPath: /mydata-data        *
            name: datavol                  *
    ```

- Kubernetes的Volume除了让一个Pod的多个容器共享文件、让容器的数据写到宿主机的磁盘上或者写文件到网络存储中，还扩展了容器配置文件集中化定义与管理(通过configMap资源对象实现)

- Kubernetes提供的Volume类型：
    1. emptDir
        - 一个emptyDir Volume是在Pod分配到Node时创建的。它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为这是Kubernetes自动分配的一个目录，当Pod从Node上移除时，emptyDir中的数据也会被永久删除。emptyDir的用途如下：
            - 临时空间，程序运行时需要的临时目录，无须永久保留。
            - 长时间任务的中间过程CheckPoint的临时保存目录。
            - 一个容器需要从另一个容器中获取数据的目录(多容器共享目录)。
    2. hostPath
        - hostPath为Pod上挂载宿主机上的文件或目录，用途如下：
            - 容器应用程序生成的日志需要永久保存时，可以使用宿主机的高速文件系统进行存储。
            - 需要访问宿主机上Docker引擎内部数据结构的容器应用时，可以通过定义hostPath为宿主机/var/lib/docker目录，使容器内部应用时可以直接访问Docker的文件系统。
        - 在使用这种类型的volume时，需要注意以下几点：
            - 在不同的Node上具有相同配置的Pod可能会因为宿主机上的目录和文件不同而导致对Volume上目录和文件的访问结果不一致。
            - 如果使用了资源配额管理，则Kubernetes无法将hostPath在宿主机上使用的资源纳入管理。

        ```
        # 示例：

        volumes:
          - name: "persistent-storage"
            hostPath:
              path: "/data"
        ```

    3. gcePersistentDisk
        - 使用这种类型的Volume表示使用谷歌公有云提供的永久磁盘(Persisten Disk,PD)存放Volume的数据，PD上的内容会被永久保存，当Pod被删除时，PD只是被卸载，但不会被删除。需要先创建一个永久磁盘才能使用gcePersistentDisk。
        - 使用acePersistentDisk有2个限制条件
            - Node(运行kubelet的节点)需要是GCE虚拟机
            - 这些虚拟机需要与PD存在于相同的GCE项目和Zone中。

        ```
        # 通过gcloud命令创建一个PD:
        $ gcloud compute disks create --size=500GB --zone=us-centrall-a my-data-disk

        volumes:
        - name: test-volume
          gcePersistentDisk:
            pdName: my-data-disk
            fsType: ext4
        ```

    4. awsElasticStore
        - 与GCE类似，该类型的Volume使用亚马逊公有云提供的EBS Volume存储数据，需要先创建一个EBS Volume才能使用awsElasticBlockStore。
        - 使用awsElasticStore有3个限制条件：
            - Node(运行kubelet的节点)需要是AWS EC2实例
            - 这些AWS EC2实例需要与EBS volumes存在于相同region和availability-zone中。
            - EBS只支持单个EC2实例mount一个volume

        ```
        # 通过aws ec2 create-volume命令创建一个EBS volume:
        $ aws ec2 create-volume --availability-zone eu-west-la --size 10 --volume-type gp2

        volumes:
        - name: test-volume
          awsElasticBlockStore:
            volumeID: aws://<availability-zone>/<volume-id>
            fsType: ext4
        ```

    5. NFS
        - 使用NFS网络文件系统提供的共享目录存储数据时，我们需要在系统中部署一个NFS Server。
        ```
        volumes:
          - name: nfs
            server: nfs-server.localhost
            path: "/"
        ```

    6. 其他类型的Volume
        - iscsi：使用iSCSI存储设备上的目录挂载到Pod中
        - flocker：使用Flocker来管理存储卷
        - gulsterfs：使用开源glusterFS网络文件系统的目录挂载到Pod中
        - rbd：使用Ceph块设备共享存储(Rados Block Device)挂载到Pod中
        - gitRepo：通过挂载一个空目录，并从GIT库clone一个git repository以供Pod使用
        - secret：一个secret volume用于为Pod提供加密的信息，你可以将定义在Kubernetes中的secret直接挂载为文件让Pod访问，secret volume是通过tmfs(内存文件系统)实现的，所以这种类型的vlume总是不会持久化的。

## Persistent Volume
- 定义在Pod上的volume，属于"计算资源"，"网络存储"是相对独立于"计算资源"而存在的一种实体资源。在使用虚拟机的情况，通常会
先定义一个网络存储，然后从中划出一个"网盘"并挂载到虚拟机上。Persistent Volume(简称PV)与之相关联的Persistent Volume Claim(简称PVC)也起到了类似的作用。

- PV可以理解成Kubernetes集群中的某个网络存储中对应的一块存储，它与Volume很类似，但是有以下区别：
    - PV只能是网络存储，不属于任何Node，但可以在每个Node上访问
    - PV并不是定义在Pod上的，而是独立于容器 Pod之外定义
    - PV目前支持的类型包括：gcePersistendDisk、AWSElasticBlockStore、AzureFile、AzureDisk、FC(Fibre Channel)、flocker、NFS、iSCSI、RBD(Rados Block Device)、CephFSck、Cinder、GlusterFS、VsphereVolume、Quobyte Volumes、VMware Photon、Portworx Volumes、ScaleIO Volumes和HostPath。

    ```
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: pv0003
    spec:
      capacity:
        storage: 5Gi
      accessModes:
        - ReadWriteOnce
      nfs:
        path: /somepath
        server: 172.17.0.2

    # ReaWriteOnce：读写权限、并且只能被单个Node挂载
    # ReadOnlyMany：只读权限、允许被多个Node挂载
    # ReadWriteMany：读写权限、允许被多个Node挂载
    ```

- PV的状态：
    - Available：空闲状态
    - Bound：已经绑定到某个PVC上
    - Released：对应的PVC已经删除，但资源还没有被集群收回
    - Failed：PV自动回收失败

- 如果某个Pod想使用某种类型的PV，则首先需要定义一个PVC对象，然后在Pod的Volume定义中引用即可：
    ```
    # 定义PVC对象
    kind: PersistentVolumeClaim
    apiVersion: v1
    metadata:
      name: myclaim
   spec:
     accessModes:
       - ReadWriteOnce
    resources:
      requests:
        storage: 8Gi

    # 引用PVC
    volumes:
      - name: mypd
        PersistentVolumeClaim:
          claimName: myclaim
    ```

## Namespace(命名空间)
- Namespace(命名空间)是Kubernetes系统中的另一个非常重要的概念，Namespace在很多情况下用于实现多租户的资源隔离。Namespace通过将集群内部的资源对象"分配"到不同的Namespace中，形成逻辑上分组的不同项目、小组或用户组，便于不同的分组在共享使用整个集群的资源的同时还能被分别管理。

- Kubernetes集群在启动后，会创建一个名为"default"的Namespace。如果不特别指明Namespace，则用户创建的pod、RC、Service都将被系统创建到这个默认的名为default的Namespace中。
```
$ kubectl get namespaces    //查看default命名空间的资源对象
$ kubectl get pods --namespace <namespace>    //查看指定命名空间的资源对象

# 创建Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: development

# 创建资源对象指定Namespace
APIVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: development      *
```

## Annotation(注解)
- Annotation与Label类似，也使用key/value键值对的形式进行定义。不同的是Label具有严格的命名规则，它定义的是Kubernetes对象的元数据(Metadata)，并且用于Label Selector，而Annotation则是用户任意定义的"附加"心扉，以便于外部工具进行查找，很多时候Kubernetes的模块自身会通过Annotation的方式标记资源对象的一些特殊信息。

- 通常用Annotation记录如下信息：
    - build信息、release信息、Docker镜像信息等，例如时间戳、release id号、PR号、镜像hash值、docker registry地址等。
    - 日志库、监控库、分析库等资源库的地址信息。
    - 程序调试工具信息、例如工具名称、版本号等。
    - 团队的联系信息，例如电话号码、负责人名称、网址等。
