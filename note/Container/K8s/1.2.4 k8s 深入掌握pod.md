
# Pod定义详解
## Pod定义文件模板的完整内容

```
apiVersion: v1
kind: Pod
metadata:
  name: string
  namespace: string
  labels:
  - name: string
  annotations:
  - name: string
spec:
  containers:
  - name: string
    image: string
    imagePullPolicy: [Always | Never | IfNotPresent]
    command: [string]
    args: [string]
    workingDir: string
    volumeMounts:
    - name: string
      mountPath: string
      readOnly: boolean
    ports:
    - name: string
      containerPort: int
      hostPort: int
      protocol: string
    env:
    - name: string
      value: string
    resources:
      limits:
        cpu: string
        memory: string
      requests:
        cpu: string
        memory: string
    livenessProbe:
      exec:
        command: [string]
      httpGet:
        path: string
        port: int
        host: string
        scheme: string
        httpHeaders:
        - name: string
          value: string
      tcpSocket:
        port: int
      initialDelaySeconds: number
      timeoutSeconds: number
      periodSeconds: number
      successThreshold: 0
      failureThreshold: 0
    securityContext:
      privileged: false
  restartPolicy: [Always | Never | OnFailure]   
  nodeSelector: object
  imagePullSecrets:
  - name: string
  hostNetwork: false
  volumes:
  - name: string
    emptyDir: {}
    hostPath:
      path: string
    secret:
      secretName: string
      items:
      - key: string
        path: string
    configMap:
      name: string
      items:
      - key: string
        path: string
```
## Pod定义文件模板属性说明
| 属性名称 | 取值类型 | 是否必选 | 取值说明 |
| :------ | :------ | :------- | :------ |
| version | String | Required | 版本号、例如v1 |
| kind | String | Required | Pod |
| metadata | Object | Required | 元数据 |
| metadata.name | String | Required | Pod的名称，命名规范需符合RFC 1035规范 |
| metadata.namespace | String | Required | Pod所属的命名空间，默认值为default |
| metadata.labels[] | List |  | 自定义标签列表 |
| metadata.annotation[] | List |  | 自定义注释列表 |
| spec | Object | Required | Pod中容器的详细定义 |
| spec.containers[] | List | Required | Pod中的容器列表 |
| spec.containers[].name | List | Required | 容器的名称，须符合RFC 1035规范 |
| spec.containers[].image | String | Required | 容器的镜像名称 |
| spec.containers[].imagePullPolicy | String |  | 获取镜像的策略，默认值为Always：表示每次都尝试重新下载镜像；Never：表示仅适用本地镜像；IfNotPresent：表示如果本地有该镜像，则适用本地镜像，本地不存在时下载镜像。 |
| spec.containers[].command[] | List |  | 容器的启动命令列表，如果不指定，则适用镜像打包时使用的启动命令 |
| spec.containers[].args[] | List |  | 容器的启动命令参数列表 |
| spec.containers[].workingDir | String |  | 容器的工作目录 |
| spec.containers[].volumeMounts[] | List |  | 挂载到容器内部的存储卷配置 |
| spec.containers[].volumeMounts[].name | String |  | 引用Pod定义的共享存储卷的名称，需使用volumes[]部分定义的共享存储卷名称 |
| spec.containers[].volumeMounts[].mountPath | String |  | 存储卷在容器内Mount的绝对路径，应少于512个字符 |
| spec.containers[].volumeMounts[].readOnly | Boolean |  | 是否为只读模式，默认值为读写模式 |
| spec.containers[].ports[] | List |  | 容器需要暴露的端口号列表 |
| spec.containers[].ports[].name | String |  | 端口的名称 |
| spec.containers[].ports[].containerPort | Int |  | 容器需要监听的端口号 |
| spec.containers[].ports[].hostPort | Int |  | 容器所在主机需要监听的端口号，默认与containerPort相同。设置hostPort时，同一台宿主机将无法启动该容器的第2份副本 |
| spec.containers[].ports[].protocol | String |  | 端口协议，支持TCP和UDP，默认值为TCP |
| spec.containers[].env[] | List |  | 容器运行前需设置的环境变量列表 |
| spec.containers[].env[].name | String |  | 环境变量的名称 |
| spec.containers[].env[].value | String |  | 环境变量的值 |
| spec.containers[].resources | Object |  | 资源限制和资源请求的设置 |
| spec.containers[].resources.limits | Object |  | 资源限制的设置 |
| spec.containers[].resources.limits.cpu | String |  | CPU限制，单位为core数，将用于dockerrun --cpu-shares参数 |
| spec.containers[].resources.limits.memory | String |  |内存限制，单位可以为MiB/GiB等，将用于docker run --memory参数  |
| spec.containers[].resources.requests | Object |  | 资源限制的设置 |
| spec.containers[].resources.requests.cpu | String |  | CPU请求，单位为core数，容器启动的初始可用数量 |
| spec.containers[].resources.requests.memory | String |  | 内存请求，单位可以为MiB、GiB等,容器启动的初始启动数量 |
| spec.volumes[] | List |  | 在该Pod上定义的共享存储卷列表 |
| spec.volumes[].name | String |  | 共享存储卷的名称，在一个Pod中每个存储卷定义一个名称，应付可RFC 1035规范。容器定义部分的container[].volumeMounts[].name将引用该共享存储卷的名称。Volume的类型包括：emptyDir、hostPath、gccPersistentDisk、awsElasticBlockStore、gitRepo、secret、nfs、iscsi、glusterfs、persistentVolumeClaim、rbd、flexVolume、cinder、cephfs、flocker、downwardAPI、fc、azureFile、configMap、vsphereVoume，可以定义多个volume，每个volume的name保持唯一 |
| spec.volumes[].emptyDir | Object |  | 类型为emptyDir的存储卷，表示与Pod同生命周期的一个临时目录，其值为一个空对象：emptyDir:{} |
| spec.volumes[].hostPath | Object |  | 类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录，通过volume[].hostPath.path指定 |
| spec.volumes[].hostPath.path | String |  | Pod所在主机的目录，将被应用于容器中mount的目录 |
| spec.volumes[].secret | Object |  | 类型为secret的存储卷，表示挂载集群预定义的secret对象到容器内部 |
| spec.volumes[].configMap | Object |  | 类型为configMap的存储卷，表示挂载集群预定义的configMap对象到容器内部 |
| spec.volumes[].livenessProbe | Object |  | 对Pod内部容器监控检查的设置，当探测无响应几次之后，系统将自动重启该容器，可以设置的方法包括：exec、httpGet和tcpSocket。对于一个容器仅需要设置一种健康检查方法 |
| spec.volumes[].livenessProbe.exec | Object |  | 对Pod内部容器监控检查的设置，exec方式 |
| spec.volumes[].livenessProbe.exec.command[] | String |  | exec方式需要指定的命令或者脚本 |
| spec.volumes[].livenessProbe.httpGet | Object |  | 对Pod内各容器健康检查，HTTPGet方式。需指定path、port |
| spec.volumes[].livenessProbe.tcpSocket | Object | Object | 对Pod内各容器健康检查，tcpSocket方式 |
| spec.volumes[].livenessProbe.initialDelaySeconds | Number |  | 容器启动完成后进行首次探测的时间，单位为s |
| spec.volumes[].livenessProbe.timeoutSeconds | Number |  | 对容器监控检查的探测等待响应的超时时间设置，单位为s，默认是为1s。超过该超时时间设置，将认为该容器不健康，将重启该容器 |
| spec.volumes[].livenessProbe.periodSeconds | Number |  | 对容器健康检查的定期探测时间设置，单位为s,默认为10s探测一次 |
| spec.restarPolicy | String |  | Pod的重启策略，默认值为Always：Pod一旦终止运行，则无论容器是如何终止的，kubelet都将重启它；onFailure：只有Pod以非零退出码终止时，kubelet才会重启该容器。如果容器正常结束(退出码为0)，则kubelet将不会重启它；Never：Pod终止后，kubelet将退出吗码报告给master，不会在重启该Pod |
| spec.nodeSelector | Object |  | 设置NodeSelector表示将该Pod调度到包含这些label的Node上，以key: value格式指定 |
| spec.imagePullSecrets | Object |  | Pull镜像时使用的secret名称，以name: secretkey格式指定 |
| spec.hostNetwork | Boolean |  | 是否使用主机网络模式，默认值为false。如果设置为true，则表示容器使用宿主机网络，不在使用Docker网桥，该Pod将无法在同一台宿主机上启动2个副本 |

# Pod的基本用法
- Docker容器中应用的运行要求：kubernetes需要创建的docker镜像以一个前台命令作为启动命令(原因是：如果创建的docker镜像是以一个linux脚本作为后台执行程序 nohup./start.sh &，kubelet创建这个容器的pod之后运行该命令，即认为pod执行结束，将立刻销毁该pod。如果该pod定义了RC，则系统根据RC定义中pod的replicas副本数量生成新的pod，一旦创建出新的pod执行完启动命令，就会陷入无限循环中)
- 对于无法使用前台执行的应用，可以使用开源工具Supervisor辅助进行前台运行的功能。Supervisor提供了一种可以同时启动多个后台应用，并保持Supervisor自身在前台执行的机制。
- 一个pod多容器示例：
```
apiVersion: v1
kind: Pod
metadata:
  name: redis-php
  labels:
    name: redis-php
spec:
  containers:
  - name: frontend
    image: kubeguide/guestbook-php-frontend:localredis
    ports:
    - containerPort: 80
  - name: redis
    image: kubeguide/redis/master
    ports:
    - containerPort: 6379
```

# 静态Pod
- 静态Pod是由kubelet进行管理的仅存在于特定Node上的Pod。不能通过API Server进行管理，无法与ReplicationController、Deployment或者DaemonSet进行关联，并且kubelet也无法对他们进行健康检查。静态Pod总是由kubelet进行创建的，并且总是在kubelet所在的Node上运行的。
- 创建静态Pod的两种方式：
  - 配置文件方式：
      - 首先需要设置kubelet的启动参数`--config=/etc/kubelet.d/`指定kubelet需要监控的配置文件所在目录，kubelet会定期扫描改目录，并且根据该目录中的.yaml或.json文件进行创建操作。重启kubelet即可。
      - 由于静态Pod无法通过APIServer直接管理，所以在Master节点尝试删除这个Pod，将会使其变成Pending状态，且不会被删除。如果要删除、只需要到其Node上将yaml文件删除即可。
  - HTTP方式：
      - 通过设置kubelet的启动参数"--mainifest-url"，kubelet将会定期从该URL地址下载定义文件，并以yaml或json文件的格式进行解析。然后创建Pod

# Pod容器共享Volume
- 在同一个Pod中的多个容器能够共享Pod级别的存储卷Volume。Volume可以被定义为各种类型，多个容器个子进行挂载操作，将一个Volume挂载为容器内部需要的目录。
    - 示例：

    ```
    apiVersion: v1
    kind: Pod
    metadata:
      name: volume-pod
    spec:
      containers:
      - name: tomcat
        image: tomcat
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: app-logs
          mountPath: /usr/local/tomcat/logs
      volumes:
      - name: app-logs
        #emptyDir: {}
        hostPath:
          path: /opt/test/data
    ```

# Pod的配置管理
应用部署的最佳实践是将应用所需的配置信息与程序进行分离，将应用打包为容器镜像后、通过环境变量或者外挂文件的方式在创建容器时进行配置注入。在k8s1.2后引入了应用配置方案-ConfigMap

## ConfigMap概述
- ConfigMap供容器使用的典型方法如下：
    - 生成为容器内的环境变量
    - 设置容器启动命令的启动参数(需设置为环境变量)
    - 以Volume的形式挂载为容器内部的文件或目录

- ConfigMap以一个或多个key:value的形式保存在Kubernetes系统中供应用使用，既可以表示一个变量的值(apploglevel=info)，也可以用于表示一个完整配置文件的内容(server.xml=<?xml...>...)

- 可以通过yaml配置文件或者直接使用`kubectl create configmap`命令行的方式来创建ConfigMap

## 创建ConfigMap资源对象
### 通过yaml创建
```
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-podconfig-nginx
  namespace: default
data:
  nginx.conf: |
   server {
       listen 80;
       include vhost/servername/bev;
       root /home/webserver/html/bev;
       access_log  /data/wwwlogs/bev.log  main;

       location / {
           index index.html;
           try_files $uri $uri/ /index.html;
           }
   }

   server.xml: |
     ...
```
### 通过kubectl命令行创建
不使用yaml文件，直接通过`kubectl create configmap`也可以创建ConfigMap，可以使用参数--from-file或--from-literal指定内容，并且可以在一行命令中指定多个参数。

- 通过`--from-file`参数从文件中创建，可以指定key的名称，也可以在一个命令行中创建包含多个key的ConfigMap。
    - `kubectl create configmap NAME --from-file=[key=]source --from-file=[key=]source`

- 通过`--from-file`参数从目录中创建，该目录下的每个配置文件名都被设置为key，文件的内容被设置为value。
    - `kubectl create configmap NAME --from-file=config-files-dir`

- 通过`--from-literal`参数从文本中创建，直接将指定的key#=value#创建为ConfigMap。
    - `kubectl create configmap NAME --from-literal=key1=value1 --from-literal=key2=value2`

## 在Pod中使用ConfigMap
### 通过环境变量方式使用ConfigMap
```
apiVersion: v1
kind: ConfigMap
metadata:
  name: cm-appvars
data:
  apploglevel: info
  appdatadir: /var/data
```
```
apiVersion: v1
kind: Pod
metadata:
  name: com-test-pod
spec:
  containers:
  - name: cm-test
    image: busybox
    command: ["/bin/sh", "-c", "env"]
    envFrom:
    - configMapRef:
      name: cm-appvars #根据cm-appvars中的key-value自动生成环境变量
```
### 通过VolumeMount
```
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-podconfig-nginx
  namespace: default
data:
  nginx-conf: |
  server-xml: |
```
```
apiVersion: v1
kind: Pod
metadata:
  name: cm-test-app
spec:
  containers:
  - name: cm-test-app
    image: kubeguide/tomcat-app:v1
    ports:
    - containerPort: 8080
    volumeMounts:                   *
    - name: nginx-tomcat-config     * 引用下边定义的volume名
      mountPath: /etc/nginx         * 挂载到容器内的目录
  volumes:                          *
  - name: nginx-tomcat-config       * 定义volume名字，方便上边引用
    configMap:                      *
      name: test-podconfig-nginx    * 引用configmap.yaml文件
```

### 使用ConfigMap的限制条件
- ConfigMap必须在Pod之前创建；
- ConfigMap受Namespace限制，只有处于相同Namespace中的Pod可以引用它；
- ConfigMap只支持可以被API Server管理的Pod使用ConfigMap。静态Pod无法引用；
- 在Pod对ConfigMap进行挂载操作时，容器内部只能挂载为"目录"，无法挂载为"文件"，如果该目录下还有文件将会被挂载覆盖。

# 在容器内获取Pod信息(Downward API)
## Downward API应用示例
- 通过Downward API在Pod容器内获取Pod的名字、IP地址、namespace等信息
- Downward API通过两种方式将Pod信息注入容器内部
    - 环境变量：用于单个变量、可以将Pod信息和Container信息注入容器内部
    - volume挂载：将数组类信息生成为文件，挂载到容器内部
- 示例：
    - 示例1：环境变量方式-将pod信息(ip 名称 ns)注入为环境变量

       ```
       # 通过Downward API将Pod的IP、名称和所在namespace注入容器的环境变量中，容器应用使用env命令将全部环境变量打印到标准输出中。
       dapi-test-pod.yaml
       apiVersion: v1
       kind: Pod
       metadata:
         name: dapi-test-pod
       spec:
         containers:
         - name: test-container
           image: busybox
           command: ["/bin/sh", "-c", "env"]
           env:
           - name: MY_POD_NAME
             valueFrom:
               fieldRef:
                 fieldPath: metadata.name
           - name: MY_POD_NAMESPACE
             valueFrom:
               fieldRef:
                 fieldPath: metadata.namespace
           - name: MY_POD_IP
             valueFrom:
               fieldRef:
                 fieldPath: status.podIP
         restartPolicy: Never

         # kubectl logs dapi-test-pod 日志中可以看到已经保存到了环境变量
      ```
          - 目前Downward API提供了以下变量：
              - metadata.name：Pod的名称，当Pod通过RC生成时，其名称是RC随机产生的唯一名称；
              - status.podIP：Pod的IP地址，之所有叫做status.podIP而非metadata.IP，是因为Pod的IP属于状态数据，而非元数据；
              - metadata.namespace：Pod所在的Namespace
    - 示例2：环境变量方式-将容器资源信息注入为环境变量

        ```
        # 通过Downward API将container的资源请求和限制信息注入容器的环境变量中，容器应用时使用printenv命令将设置的资源请求和资源限制环境变量打印到标准输出中
        dapi-test-pod-container-vars.yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: daip-test-pod-container-vars
        spec:
          containers:
          - name: test-container
            image: busybox
            imagePullPolicy: Never
            command: ["sh", "-c"]
            args:
            - while true; do
                echo -en '\n';
                printenv MY_CPU_REQUEST MY_CPU_LIMIT;
                printenv MY_MEM_REQUEST MY_MEM_LIMIT;
                sleep 3600;
              done;
            resources:
              requests:
                memory: "32Mi"
                cpu: "12m"
              limits:
                memory: "64Mi"
                cpu: "250m"
            env:
            - name: MY_CPU_REQUEST
              valueFrom:
                resourceFieldRef:
                  containerName: test-container
                  resource: requests.cpu
            - name: MY_CPU_LIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: test-container
                  resource: limits.cpu    
            - name: MY_MEM_REQUEST
              valueFrom:
                resourceFieldRef:
                  containerName: test-container
                  resource: requests.memory
            - name: MY_MEM_LIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: test-container
                  resource: limits.memory
          restartPolicy: Never

        # kubectl logs daip-test-pod-container-vars可以查看到变量
        ```
        - 目前valueFrom这种特殊的Downward API语法可以将容器的资源请求和资源限制设置为容器内部的环境变量：
            - request.cpu：容器的CPU请求值
            - limits.cpu：容器的CPU限制值
            - requests.memory：容器的内存请求值
            - limits.memory：容器的内存限制值

    - 示例3：volume挂载方式
        - 通过Downward API将Pod的Label，annotation列表通过volume挂载为容器内的一个文件，容器应用使用echo命令将文件内容打印到标准输出中。

        ```
        dapi-test-pod-volume.yaml
        apiVersion: v1
        kind: Pod
        metadata:
          name: dapi-test-pod-volume
          labels:
            zone: us-est-coast
            cluster: test-cluster1
            rack: rack-22
          annotations:
            build: two
            builder: testuser
        spec:
          containers:
          - name: test-container
            image: busybox
            imagePullPolicy: Never
            command: ["sh", "-c"]
            args:
            - while true; do
                if [[ -e /etc/labels ]]; then
                  echo -en '\n\n'; cat /etc/labels; fi;
                if [[ -e /etc/annotations ]]; then
                  echo -en '\n\n'; cat /etc/annotations; fi;
                sleep 3600;
              done;
            volumeMounts:
            - name: podinfo
              mountPath: /etc
              readOnly: false
          volumes:
          - name: podinfo
            downwardAPI:
              items:
              - path: "labels"
                fieldRef:
                  fieldPath: metadata.labels
              - path: "annotations"
                fieldRef:
                  fieldPath: metadata.annotations
        ```

## Downward API应用范围
在某些集群中，集群中的每个节点都需要将自身的标识(ID)及进程绑定的IP地址等信息提前写入配置中，进程启动时读取这些信息发布到某个服务注册中心，以实现集群节点自动发现功能。

# Pod生命周期和重启策略
## Pod声明周期
Pod在整个生命周期过程中被系统定义为各种状态，如下表
| 状态值 | 描述 |
| :---- | :--- |
| Pending | API Server已经创建该Pod，但Pod内还有一个或多个容器的镜像没有创建，包括正在下载镜像的过程 |
| Running | Pod内所有容器均已创建，且至少有一个容器处于运行状态，正在启动状态或正在重启状态 |
| Successed | Pod内所有容器均已成功执行退出，且不会再重启 |
| Failed | Pod内所有容器均已退出，但至少有一个容器退出为失败状态 |
| Unknown | 由于某种原因无法获取该Pod状态，可能由于网络通信不畅导致 |

## pod重启策略
Pod的重启策略(RestartPolicy)应用于Pod内的所有容器，并且仅在Pod所处的Node上由kubelet进行判断和重启操作。当某个容器异常退出或者健康检查失败时，kubelet将根据RestartPolicy的设置进行相应操作

- Pod的重启策略包括三种，默认为Always:
    - Always：当容器失效时，由kubelet自动重启该容器
    - OnFailure：当容器终止运行且退出代码不为0时，由kubelet自动重启该容器
    - Never：不论容器运行状态如何，kubelet都不会重启该容器

- 每种控制器对Pod的重启策略要求：
    - RC和DaemonSet：必须设置为Always，需要保证该容器持续运行
    - Job：OnFailure或Never，确保容器执行完成后不再重启
    - Kubelet：在Pod失效时自动重启它，无论将RestartPolicy设置为什么值，也不会对Pod进行健康检查

- 状态场景转换

| Pod包含的容器数 | Pod当前的状态 | 发生事件 | RestartPolicy=Always | =OnFailure | =Never |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 包含1个容器 | Running | 容器成功退出 | Running | Succeeded | Succeeded |
| 包含1个容器 | Running | 容器失败退出 | Running | Running | Failed |
| 包含2个容器 | Running | 1个容器失败退出 | Running | Running | Running |
| 包含2个容器 | Running | 容器被OOM杀掉 | Running | Running | Failed |

# Pod健康检查
对于Pod的健康状态检查可以通过两类探针来检查：LivenessProbe和ReadinessProbe
- LivenessProbe：用于判断容器是否存活(running状态)，如果LivenessProbe探针探测到容器不健康，则kubelet将杀掉该容器，并根据容器的重启策略做相应的处理。如果一个容器不包含LivenessProbe探针，那么kubelet认为该容器的LivenessProbe探针返回的值永远是"Success"
- ReadinessProbe：用于判断容器是否启动完成(ready状态)，可以接受请求。如果ReadinessProbe探针检测到失败，则Pod的状态将被修改。Endpoint Controller将从Service的Endpoint中删除包含该容器所在Pod的Endpoint

kubelet定期执行LivenessProbe探针来诊断容器的健康状态，LivenessProbe有三种实现方式：
- ExecAction：在容器内部执行一个命令，如果该命令的返回码为0，则表明容器健康。
- TCPSocketAction：通过容器的IP地址和端口号执行TCP检查，如果能够建立TCP连接，则表明容器健康。
- HTTPGetAction：荣国容器的IP地址、端口号及路径调用HTTP Get方法，如果相应大状态吗大于等于200且小于400，则认为容器状态健康。

对于每种探测方式，都需要设置initialDelaySeconds和timeoutSeconds两个参数，他们的还以分别如下：
- initialDelaySeconds：启动容器后进行首次健康检查的等待时间，单位为s。
- timeoutSeconds：健康检查发送请求后等待相应的超时时间，单位为s。当超时发生时，kubelet会认为容器已经无法提供服务，将会重启该容器。

# 玩转Pod调度
在kubernetes系统中，Pod在大部分场景下都只是容器的载体而已，通常需要通过Deployment、DaemonSet、RC、Job等对象来完成一组Pod的调度与自动控制功能

## Deployment/RC：全自动调度
Deployment或RC的主要功能之一就是自动部署一个容器应用的多份副本，以及持续监控副本的数量，在集群内始终维持用户指定的副本数量。
```
nginx-deployment.yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx-deoplyment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
```

## NodeSelector：定向调度
Kubernetes Master上的Scheduler服务(kube-scheduler进程)负责实现Pod的调度，整个调度过程通过执行一系列发杂的算法，最终为每个Pod计算出一个最佳的目标及几点，这一过程都是自动完成的，通常我们无法知道Pod最终会被调度到那个节点上。在实际情况中，也可能需要将Pod调度到指定的一些Node上，可以通过Node的标签(Label)和Pod的nodeSelector属相相匹配来实现。
1. 首先通过kubectl label命令给目标Node上打上标签；
    ```
    $ kubectl label node node <node-name> <label-key>=<label=value>
    $ kubectl label node node kis-node2 zone=north
    ```

2. 然后，在Pod的定义上加上NodeSelector的设置；
```
redis-master-controller.yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: redis-master
  labels:
    name: redis-master
spec:
  replicas: 1
  selector:
    name: redis-master
  template:
    metadata:
      labels:
        name: redis-master
    spec:
      containers:
      - name: master
        image: kubeguide/redis-master
        ports:
        - containerPort: 6379
      nodeSelector:
        zone: north
```

> 如果给多个Node都定义了相同的标签，kubernetes将会根据调度算法从这组Node中挑选一个可用的Node进行Pod调度。

> 通过基于Node标签的调度方式，可以把集群中具有不同特点的Node贴上不同的标签，例如"role=fronted role=backend role=database"等标签，在部署应用时就可以根据应用的需求设置NodeSelector来指定Node范围的调度。

> 除了自定义的的Node标签、kubernetes还有Node预定义的一些标签：
> > kubernetes.io/hostname

> > failure-domain.beta.kubernetes.io/zone

> > failure-domain.beta.kubernetes.io/region

> > beta.kubernetes.io/instance-type

> > beta.kubernetes.io/os

> > beta.kubernetes.io/arch

- NodeSelector通过标签的方式，简单地实现了限制Pod所在节点的方法。亲和性调度机制则极大地扩展了Pod的调度能力，主要的增强功能如下：
    - 更具表达力(不仅仅是"符合全部"的简单情况)；
    - 可以使用软限制、优先采用等限制方式，代替之前的硬限制，这样调度器在无法满足优先需求的情况下，会退而求其次，继续运行该Pod；
    - 可以依据节点上正在运行的其他Pod的标签来进行限制，而非节点本身的标签。这样就可以定义一种规则来描述Pod之前的亲和或互斥关系；
    - 亲和性调度功能包括节点亲和性(NodeAffinity)和Pod亲和性(PodAffinity)两个维度的设置。节点亲和性与NodeSelector类似，增强了上述前两点的优势；Pod的亲和与互斥限制规则通过Pod标签而不是节点标签来实现。

## NodeAffinity：Node亲和性调度
NodeAffinity意为Node亲和性的调度策略，是用于替换NodeSelector的全新调度策略。目前有两种节点亲和性表达：
- requiredDuringSchedulingIgnoredDuringExecution：必须满足指定的规则才可以调度Pod到Node上(功能与nodeSelector很像，但是使用的是不同的语法)，相当于硬限制。
- PreferreDruingSchedulingIgnoredDuringExecution：强调优先满足指定规则，调度器会尝试调度Pod到Node上，但并不强求，相当于软限制。多个优先级规则还可以设置权重(weight)值，已定义执行的先后顺序。

> IgnoredDuringExecution的意思是：如果一个Pod所在的节点在Pod运行期间标签发生了变更，不再符合该Pod的节点亲和性需求，则
该系统将忽略Node上Label的变化，该Pod能继续在该节点运行。

```
apiVsersion: v1
kind: Pod
metadata:
  name: with-mode-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: beta.kubernetes.io/arch
            operator: In
            values:
            - amd64
      preferreDruingSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: disk-type
            operator: In
            values:
            - ssd
  containers:
  - name: with-node-affinity
    image: gcr.io/google_containers/pause:2.0
```
> NodeAffinity语法支持的操作符包括In、NotIn、Exists、DoesNotExist、Gt、Lt。虽然没有节点排斥的功能，但是用NotIn和DoesNotExist就可以实现互斥功能。

- NodeAffinity规则设置的注意事项：
    - 如果同时定义了nodeSelectory和nodeAffinity，那么必须两个条件都得到满足，Pod才能最终运行在指定的Node上。
    - 如果nodeAffinity制定了多个nodeSelectorTerms，那么只需要其中一个能够匹配成功即可。
    - 如果nodeSelectorTerms中有多个mathExpressions，则一个节点必须满足所有matchExpressions才能运行改Pod。

## PodAffinity：Pod亲和与互斥调度策略
- Pod间的亲和和互斥从Kubernetes v1.4版本开始引入。它是根据节点上正在运行的Pod的标签而不是节点的标签进行判断和调度，要求对节点和Pod两个条件进行匹配。这种规则可以描述为：如果在具有标签X的Node上运行了一个或者多个符合条件Y的Pod，那么Pod应该(如果是互斥的情况，那么就变成拒绝)运行在这个Node上。
    - X指一个集群中的节点、机架、区域等概念，通过kubernetes内置节点标签中的key来进行声明。这个key的名字为topologyKey，意为表达节点所属的topology范围
        - kubernetes.io/hostname
        - failure-domain.beta.kubernetes.io/zone
        - failure-domain.beta.kubernetes.io/region
    - Y指的是一个或者全部命名空间中的一个LabelSelector(与节点不同的是，Pod是属于某个命名空间)

- 和节点亲和相同，Pod亲和和互斥的条件设置也是requiredDuringSchedulingIgnoredDuringExecution和preferreDruingSchedulingIgnoredDuringExecution。Pod亲和性定义于PodSpec的affinity字段下的podAffinity子字段里。Pod间的互斥性则定义于同一层次的podAntiAffinity字字段中。
    - 示例说明Pod间的亲和性和互斥性策略设置，参照目标pod
    ```
    # 创建一个带有标签的Pod，后面将使用pod-flag这个Pod作为亲和性和互斥性的目标Pod。
    pod-flag.yaml

    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-flag
      labels:
        security: "S1"
        app: "nginx"
    spec:
      containers:
      - name: nginx
        image: nginx
    ```

    - Pod的亲和性调度
    ```
    # 定义亲和标签security=S1，对应上面的参照Pod，topologyKey的值被设置为kubernetes.io/hostname。
    pod-affinity.yaml

    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-affinity
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: security
                operator: In
                values:
                - S1
            topologyKey: kubernetes.io/hostname
      containers:
      - name: with-pod-affinity
        image: gcr.io/google_containers/pause:2.0
    ```
    > 启动后可以看到pod-affinity和pod-flag处于同一个node

    - Pod的互斥性调度
    ```
    # 创建一个新Pod与security=S1的Pod为同一个zone，但是不与app=nginx的Pod为同一个Node。
    pod-anti_affinity.yaml

    apiVersion: v1
    kind: Pod
    metadata:
      name: anti-affinity
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
            matchExpressions:
            - key: security
             operator: In
             values:
             - S1
          topologyKey: failure-domain.beta.kubernetes.io/zone
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
            matchExpressions:
            - key: app
             operator: In
             values:
             - nginx
          topologyKey: kubernetes.io/hostname
      containers:
      - name: anti-affinity
        image: gcr.io/google_containers/pause:2.0
    ```

### topologyKye的赋值限制及规则设置
- topologyKey赋值限制：
    - 原则上，topologyKey可以使用任何合法的标签key赋值；
    - 在pod亲和性和RequiredDurintScheduling的pod互斥性的定义中，不允许使用空的topologyKey；
    - 如果Admission controller包含了LimitPodHardAntiAffinityTopology，那么针对RequiredDuringScheduling的Pod互斥性定义就被限制为kubernetes.io/hostname，要使用自定义的topologyKey，就要改写或禁用该控制器；
    - 在PreferredDuringScheduling类型的Pod互斥性定义中，空的topologyKey会被解释为kubernetes.io/hostname、failure-domain.beta.kubernetes.io/zone及failure-domain.beta.kubernetes.io/region的组合；
    - 如果不是上述情况，就可以采用任意合法的topologyKey了。

- topologyKey规则设置的注意事项：
    - 除了设置Label Selector和topologyKey，用户还可以制定namespace列表来进行限制，使用Label Selector对namespace进行选择。namespace的定义和Label Selector以topologyKey同级。省略namespace的使者，表示使用定义了affinity、anti-affinity的Pod所在的namespace。如果namespace设置为空值("")，则表示所有namespace；
    - 在所有关联requiredDuringIgnoreDuringExecution的matchExpressions全都满足之后，系统才能将Pod调度到某个Node上。

## Taints和Tolerations(污点和容忍)
- NodeAffinity节点亲和性是在Pod上定义的一种属性，使得Pod能够被调度到某些Node上运行(优先选择或强制要求)。Taint则正好相反，它让Node拒绝Pod的运行。
- Taint需要和Tolerations配合使用，让Pod避开那些不合适的Node。在Node上设置一个或多个Taint之后，除非Pod明确声明能够容忍这些污点，否则无法在这些Node上运行。Toleration是Pod的属性，让Pod能够(注意只是能够，而非必须)运行在标注了Taint的Node上。

- `kubectl taint node node1 key=value:NoSchedule`设置node1加上一个Taint。Taint效果是禁止调度。这以为着除非Pod明确声明可以容忍这个Taint，否则就不会被调度到node1上。
- 在Pod上声明Toleration设置为可以容忍(Tolerate)具有该Taint的Node，使得该Pod能够被调度到node1上。
    ```
    tolerations:
    - key: "key"
      operator: "Equal"
      value: "value"
      effect: "NoSchedule"

    或者

    tolerations:
    - key: "key"
      operator: "Exists"
      effect: "NoSchedule"
    ```

    - Pod的Toleration声明中的key和effect需要与Taint的设置保持一致，并且满足一下条件之一：
        - operator的值是Exists(无须指定value)；
        - operator的值时Equal并且value相等。
    - 如果不指定operator，则默认值为Equal。另外还有两个特例：
        - 空的key配合Exists操作符能够匹配所有的键和值；
        - 空的effect匹配所有的effect。
    - effect取值：
        - NoSchedule  不调度
        - PreferNoSchedule  优先调度
        - NoExecute

- 系统允许在同一个Node上设置多个Taint，也可以在Pod上设置多个Toleration。kubernetes调度器处理多个Taint和Toleration的逻辑顺序为：首先列出节点中所有的Taint，然后忽略Pod的Toleration能够匹配的部分，剩下的没有忽略掉的Taint就是对Pod的效果，特殊情况如下：
    - 如果剩余的Taint中存在effect=NoSchedule，则调度器不会把该Pod调度到这一节点上；
    - 如果剩余Taint中没有NoSchedule效果，但是有PreferNoSchedule效果，则调度器会尝试不把这个Pod指派个这个节点；
    - 如果剩余Taint的效果有NoExecute的，并且这个Pod已经存在该节点上运行，则会被驱逐；如果没有在该节点上运行，也不会再被调度到该节点上。

### 示例一：独占节点
如果想要拿出一部分节点，专门给一些特定应用使用，则可以为节点添加这样的Taint。eg：`kubectl taint node nodename dedicated=groupName:NoSchedule`，然后给这些应用的Pod加入对应的Toleration，这样带有合适Toleration的Pod就会被允许同时用其他节点一样使用有Taint的节点

### 示例二：具有特殊硬件设备的节点
在集群里可能有一小部分节点安装了特殊的硬件设备，用户可以通过Taint把不需要占用这类硬件的Pod排除在外

### 示例三：定义Pod驱逐行为，以应对节点故障(Alpha版本功能)
effect=NoExecute这个Taint效果对节点上正在运行的Pod有以下影响：
- 没有设置Toleration的Pod会被立刻驱逐；
- 配置了对应Toleration的pod，如果没有为tolerationSeconds赋值，则会一直留在这个节点中；
- 配置了对应Toleration的Pod且指定了tolerationSeconds值，则会在指定时间后驱逐；
- 标记节点故障

## DaemonSet：在每个Node上调度一个Pod
DaemonSet是kubernetesv1.2版本新增的一种资源对象，用于管理在集群中每个Node上仅运行一份Pod的副本实例，常见应用如下：
- 用在每个Node上运行一个GlusterFs存储或者Ceph存储的Daemon进程；
- 用在每个Node上运行一个日志采集程序，例如Fluentd或者Logstach；
- 用在每个Node上运行一个性能加农程序，采集改Node的运行性能数据，例如Prometheus Node Exporter、collectd、New Relic agent或者Ganglia gmond等。

## Job：批处理调度

## Cronjob：定时任务:q!


## 自定义调度

# Init Container(初始化容器)


# Pod的升级和回滚


# Pod的扩容和缩容


# 使用StatefulSet搭建MongoDB集群
